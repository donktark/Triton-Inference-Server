# Triton-Inference-Server
Requirements : Docker Container, GPU device, Linux OS  
## 1. Model
Model : Bloom_560m    
[ğŸ¤—Hugging Face - BLOOM_560m](https://huggingface.co/bigscience/bloom-560m?library=true)

## 2. Model Repository & Configurations
### Repository êµ¬ì„±
Triton Inference Serverì˜ ê¸°ë³¸ì ì¸ ëª¨ë¸ ë¦¬í¬ì§€í† ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•œë‹¤.
```bash
model_repository
â”‚
â””â”€â”€ bloom-560m #ëª¨ë¸ëª…
    â”œâ”€â”€ config.pbtxt #ëª¨ë¸ config
		â””â”€â”€ 1 #ëª¨ë¸ ë²„ì „
        â””â”€â”€ model.py #ëª¨ë¸ ì €ì¥ìœ„ì¹˜
```

### Model íŒŒì¼
model.pyëŠ” ëª¨ë¸ì„ ì§€ì •í•´ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì‚¬ìš©í•  Python Backendë¥¼ ì‚¬ìš©í•  ì§€ py íŒŒì¼ì„ ë„£ëŠ”ë‹¤.  
(ìƒì„¸ ì½”ë“œëŠ” [model.py](/model_repository/bloom-560m/1/model.py) ì°¸ì¡°)
```python
class TritonPythonModel:
    def initialize(self, args):
        self.generator = pipeline("text-generation", model="bigscience/bloom-560m", device = 0)

    def execute(self, requests):
            ....

    def finalize(self, args):
         self.generator = None
```
### Config íŒŒì¼
Auto-configurationì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ í¬ë§·ì´ë¼ë©´ config.pbtxtë¥¼ ìƒì„±í•˜ì—¬ ì‘ì„±í•œë‹¤.  
(ìƒì„¸ì½”ë“œëŠ” [config.pbtxt](/model_repository/bloom-560m/config.pbtxt) ì°¸ì¡°)
```yaml
name : "bloom-560m"
backend : "python"
max_batch_size : 0
input [
    ...
]
output [
    ...
]
instance_group [
    ...
]
```
## 3. Triton Inference Server ì‹¤í–‰
Triton Serverì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ `dockerfile`íŒŒì¼ì„ ì´ìš©í•œë‹¤.
ë²„ì „ê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‘ì„±í•œ ë’¤ì— `build` ì»¤ë§¨ë“œë¥¼ í†µí•´ ë¹Œë“œí•œë‹¤.
```yaml
# Dockerfile
FROM nvcr.io/nvidia/tritonserver:24.01-py3
RUN pip install transformers torch

# build command
docker build -t triton_image .
```
shellì— ì•„ë˜ ì»¤ë§¨ë“œë¥¼ ì…ë ¥í•´ì„œ ë„ì»¤ ì´ë¯¸ì§€ë¥¼ ì‹¤í–‰ì‹œí‚¨ë‹¤.
```bash
docker run --gpus all --name triton_server --rm -p8000:8000 -p8001:8001 -p8002:8002 -v /home/linux/model_repository/:/models triton_image tritonserver --model-repository=/models
```
## 4. Request Inference LLM Model
Python í˜¹ì€ cURL ë“±ìœ¼ë¡œ Inferenceë¥¼ í•œë‹¤.  
(ìƒì„¸ì½”ë“œëŠ” [run_inference.ipynb](/run_inference.ipynb) í˜¹ì€ [run_inference.py](/run_inference.py) ì°¸ì¡°)
```python
import requests

url = 'http://localhost:8000/v2/models/bloom-560m/infer'

sentence = [ "ì…ë ¥ í…ìŠ¤íŠ¸" ]
data = {
    'inputs': [
        ...
    ],
    'params.': [
        ...
    ]
}
response = requests.post(url, json=data)
answer = response.json()
print(answer)
print('\n ğŸ‘»', answer['outputs'][0]['data'])
```
pyë¡œ ì‘ì„±í•œ íŒŒì¼ì€ `python3 run_inference.py`ë¥¼ í†µí•´ Inferenceí•  ìˆ˜ ìˆë‹¤.
## 5. ëª¨ë¸ ì¶”ë¡  ì„±ëŠ¥ ìˆ˜ì¹˜í™”
Triton sdkì— ë‚´ì¥ëœ Perf Analyzerë¥¼ í†µí•´ ëª¨ë¸ì˜ Inferencing ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸ë¥¼ í•´ì„œ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.
```bash
$ docker run --rm --net=host -it --name triton_sdk \
-v /home/linux/perf_results/:/workspace/ \
nvcr.io/nvidia/tritonserver:24.01-py3-sdk bash
```
ì•„ë˜ ì»¤ë§¨ë“œë¥¼ í†µí•´ ì´ 20ê°œê¹Œì§€ì˜ í…ŒìŠ¤íŠ¸ ì¶”ë¡ ì„ ë‹¨ê³„ì ìœ¼ë¡œ ëŠ˜ë¦¬ë©´ì„œ ì¶”ë¡ í•œë‹¤.  
ìµœì¢…ì ìœ¼ë¡œ ë‚˜ì˜¨ ê²°ê³¼ëŠ” perf_results.csv íŒŒì¼ë¡œ ì €ì¥ëœë‹¤.
```bash
$ perf_analyzer -m bloom-560m -u localhost:8001 -i grpc --concurrency-range 1:20 --input-data random --shape=input:5 --max-trial 5 --string-length=10 -f perf_results
```
## 6. ëª¨ë¸ ì¶”ë¡  ì„±ëŠ¥ ì‹œê°í™”
csvíŒŒì¼ì„ í†µí•´ ì–»ì–´ì§„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œë¥¼ ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.
```python
perf_df = pd.read_csv("/home/linux/perf_results/perf_results.csv", index_col=0).sort_values(by='Concurrency')
perf_df

fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12.5,8))
plt.ticklabel_format(axis='y', useOffset=False, style='plain')

#ì§€í‘œ 4ê°œë§Œ í‘œì‹œ, í…ŒìŠ¤íŠ¸ ê²°ê³¼ëŠ” 12ê°œì˜ ì§€í‘œë¥¼ í¬í•¨
for i in list(range(0, 4)):
    subplot = eval(f'ax{ i % 2 + 1}')[0 if i < 2 else 1]
    subplot.plot(perf_df.iloc[:,i])
    subplot.set_title(perf_df.iloc[:,i].name)
```
<p align='center'> <img src="./images//perf_analyzer_result.bmp" width="70%" height="70%"/> </p>

## 7. Triton Server ëª¨ë‹ˆí„°ë§
Prometheusì™€ Grafana ë„ì»¤ ì»¨í…Œì´ë„ˆë¥¼ ì´ìš©í•´ Triton serverê°€ ë‚´ë³´ë‚´ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ì—°ê²°í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ì´ ê°€ëŠ¥í•˜ë‹¤. 
`prometheus.yml`ì„ ì•„ë˜ì²˜ëŸ¼ ì‘ì„±í•œë‹¤.
```yaml

global:
   scrape_interval: 15s
scrape_configs:
 - job_name: 'prometheus'
   static_configs:
           - targets:
                 - 123.456.789.000:8002 # Tritonì´ ì‘ë™í•˜ê³  ìˆëŠ” IP, 8002ë²ˆ ë§¤íŠ¸ë¦­ìŠ¤ í¬íŠ¸
```
### Prometheus
Prometheus ì—°ê²°ì„ ìœ„í•œ ë„ì»¤ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„± í›„ ì§„í–‰í•œë‹¤.
```bash
docker network create mynetwork

docker run --network=mynetwork -p 9090:9090 -v /home/linux:/etc/prometheus/ --name prometheus -d prom/prometheus --config.file=/etc/prometheus/prometheus.yml

curl http://localhost:9090
 ```
 ### Grafana
 ```bash
docker run --network=mynetwork -d --name=grafana -p 3000:3000 grafana/grafana

curl http://localhost:3000
 ```
 Grafanaì— Prometheusë¥¼ ì—°ê²°í•˜ì—¬ ëª¨ë‹ˆí„°ë§ ì§€í‘œë¥¼ ì—°ë™í•œë‹¤.

<p align='center'> <img src="./images/prometheus_connection.bmp" width="70%" height="70%"/> </p>

### Grafana Dashboard
ì‚¬ì§„ì— ë‚˜íƒ€ë‚œ ê²ƒ ì™¸ì—ë„ ë” ë§ì€ ë¦¬ì†ŒìŠ¤ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤.

<p align='center'> <img src=".images\grafana_dashboard.bmp" width="90%" height="90%"/> </p>